.. |br| raw:: html

  <br/>
  
References
==========

**Summary:** This a is non-exhaustive list of references for this component.

|

.. contents:: **Table of Contents**

|

Most of the methods below use *Self-Supervised Learning* (for the stage of Preprocessing) in order to achieve *representation disentanglement*.


Using model-based Reinforcement Learning
----------------------------------------

**2018**

- `Recall Traces: Backtracking Models for Efficient Reinforcement Learning <https://arxiv.org/pdf/1804.00379.pdf>`_

**2019**

- `Task-Agnostic Dynamics Priors for Deep Reinforcement Learning <https://arxiv.org/pdf/1905.04819.pdf>`_

Using Dynamical Systems
-----------------------

**2016**

- `Kernel Learning for Dynamic Texture Synthesis <https://www.researchgate.net/profile/Shujian-Yu/publication/308772804_Kernel_Learning_for_Dynamic_Texture_Synthesis/links/5aa94130458515178818a7c7/Kernel-Learning-for-Dynamic-Texture-Synthesis.pdf>`_

**2019**

- `VideoBERT: A Joint Model for Video and Language Representation Learning <https://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning_ICCV_2019_paper.pdf>`_
- `Self-Supervised Learning by Cross-Modal Audio-Video Clustering <https://arxiv.org/pdf/1911.12667.pdf>`_

**2021**

- `Discovering State Variables Hidden in Experimental Data <https://arxiv.org/pdf/2112.10755.pdf>`_

**2022**

- `Automated discovery of fundamental variables hidden in experimental data <http://generalroboticslab.com/assets/files/NSV_paper.pdf>`_
